{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas para modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "- Supongamos que tenemos un conjunto de datos de 100 personas que han sido sometidas a pruebas para determinar si tienen cáncer o no. \n",
    "- El modelo de clasificación utilizado genera las siguientes predicciones:\n",
    "\n",
    "- 60 personas realmente tienen cáncer (positivos reales).\n",
    "- 40 personas no tienen cáncer (negativos reales).\n",
    "\n",
    "El modelo hace las siguientes predicciones:\n",
    "\n",
    "- Predice correctamente que 50 personas tienen cáncer.\n",
    "- Predice incorrectamente que 10 personas tienen cáncer (falsos positivos).\n",
    "- Predice incorrectamente que 5 personas no tienen cáncer (falsos negativos).\n",
    "- Predice correctamente que 35 personas no tienen cáncer.\n",
    "\n",
    "**¿Cómo representar de manera resumida estos resultados?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| ID | Real: Cáncer | Predicción: Cáncer |\n",
    "|----|--------------|--------------------|\n",
    "| 1  | Sí           | Sí                 |\n",
    "| 2  | No           | No                 |\n",
    "| 3  | Sí           | Sí                 |\n",
    "| 4  | No           | Sí                 |\n",
    "| 5  | Sí           | No                 |\n",
    "| 6  | No           | No                 |\n",
    "| 7  | Sí           | Sí                 |\n",
    "| 8  | Sí           | Sí                 |\n",
    "| ...  | ...         | ...           |\n",
    "| 100 | No           | Sí                 |\n",
    "\n",
    "### Descripción de los Datos\n",
    "- **ID**: Identificador único para cada persona.\n",
    "- **Real: Cáncer**: Etiqueta real indicando si la persona tiene cáncer (Sí) o no (No).\n",
    "- **Predicción: Cáncer**: Predicción del modelo indicando si la persona tiene cáncer (Sí) o no (No).\n",
    "\n",
    "### Explicación de los Datos\n",
    "- **ID 1**: La persona realmente tiene cáncer y el modelo predice correctamente (VP).\n",
    "- **ID 2**: La persona no tiene cáncer y el modelo predice correctamente (VN).\n",
    "- ** etc **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusión\n",
    "\n",
    "La matriz de confusión basada en este escenario se vería así:\n",
    "\n",
    "$\n",
    "\\begin{array}{c|cc}\n",
    "& \\text{Predicción: Cáncer} & \\text{Predicción: No Cáncer} \\\\\n",
    "\\hline\n",
    "\\text{Real: Cáncer} & \\text{VP = 50} & \\text{FN = 10} \\\\\n",
    "\\text{Real: No Cáncer} & \\text{FP = 5} & \\text{VN = 35} \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Métricas de la Matriz de Confusión\n",
    "Ahora, calculemos las métricas clave utilizando los valores de la matriz de confusión.\n",
    "\n",
    "1. **Precisión (Accuracy)**:\n",
    "   La precisión mide la proporción de predicciones correctas entre el total de predicciones.\n",
    "   \n",
    "   $\n",
    "   \\text{Precisión} = \\frac{\\text{VP} + \\text{VN}}{\\text{Total}} = \\frac{50 + 35}{100} = 0.85\n",
    "   $\n",
    "\n",
    "2. **Precisión Positiva (Precision)**:\n",
    "   La precisión positiva mide la proporción de verdaderos positivos entre todas las predicciones positivas.\n",
    "   \n",
    "   $\n",
    "   \\text{Precisión Positiva} = \\frac{\\text{VP}}{\\text{VP} + \\text{FP}} = \\frac{50}{50 + 5} = 0.91\n",
    "   $\n",
    "\n",
    "3. **Sensibilidad (Recall) o Tasa de Verdaderos Positivos (TPR)**:\n",
    "   La sensibilidad mide la proporción de verdaderos positivos entre todos los positivos reales.\n",
    "  \n",
    "   $\n",
    "   \\text{Sensibilidad} = \\frac{\\text{VP}}{\\text{VP} + \\text{FN}} = \\frac{50}{50 + 10} = 0.83\n",
    "   $\n",
    "\n",
    "4. **Especificidad (Specificity) o Tasa de Verdaderos Negativos (TNR)**:\n",
    "   La especificidad mide la proporción de verdaderos negativos entre todos los negativos reales.\n",
    "   \n",
    "   $\n",
    "   \\text{Especificidad} = \\frac{\\text{VN}}{\\text{VN} + \\text{FP}} = \\frac{35}{35 + 5} = 0.88\n",
    "   $\n",
    "\n",
    "5. **Puntuación F1 (F1 Score)**:\n",
    "   La puntuación F1 es la media armónica de la precisión positiva y la sensibilidad.\n",
    "   \n",
    "   $\n",
    "   \\text{F1 Score} = \\frac{2 \\cdot \\text{Precisión Positiva} \\cdot \\text{Sensibilidad}}{\\text{Precisión Positiva} + \\text{Sensibilidad}} = \\frac{2 \\cdot 0.91 \\cdot 0.83}{0.91 + 0.83} = 0.87\n",
    "   $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ¿Qué significan estas métricas?\n",
    "\n",
    "### 1. Precisión (Accuracy)\n",
    "**Definición**: La precisión es la proporción de predicciones correctas (verdaderos positivos y verdaderos negativos) entre el total de predicciones realizadas.\n",
    "\n",
    "**Fórmula**:\n",
    "\n",
    "$\n",
    "\\text{Precisión} = \\frac{\\text{VP} + \\text{VN}}{\\text{Total}}\n",
    "$\n",
    "\n",
    "**Explicación Conceptual**: \n",
    "La precisión es una métrica general que indica qué tan a menudo el clasificador es correcto en sus predicciones. Si un modelo tiene alta precisión, significa que la mayoría de las veces predice correctamente tanto los casos positivos como los negativos. Sin embargo, puede no ser suficiente en casos de clases desbalanceadas, ya que puede ser alta simplemente porque la clase mayoritaria está siendo predicha correctamente.\n",
    "\n",
    "### 2. Precisión Positiva (Precision)\n",
    "**Definición**: La precisión positiva mide la proporción de verdaderos positivos entre todas las predicciones positivas.\n",
    "\n",
    "**Fórmula**:\n",
    "\n",
    "$\n",
    "\\text{Precisión Positiva} = \\frac{\\text{VP}}{\\text{VP} + \\text{FP}}\n",
    "$\n",
    "\n",
    "**Explicación Conceptual**: \n",
    "\n",
    "- La precisión positiva nos dice cuán precisa es la predicción positiva del modelo. \n",
    "- Es especialmente importante cuando las consecuencias de un falso positivo son severas. \n",
    "- En otras palabras, un modelo con alta precisión positiva produce pocos falsos positivos. \n",
    "- Esta métrica es crucial en contextos donde una predicción positiva incorrecta puede tener graves consecuencias.\n",
    "\n",
    "### 3. Sensibilidad (Recall) o Tasa de Verdaderos Positivos (TPR)\n",
    "**Definición**: La sensibilidad, también conocida como recall o tasa de verdaderos positivos, mide la proporción de verdaderos positivos entre todos los casos positivos reales.\n",
    "\n",
    "**Fórmula**:\n",
    "\n",
    "$\n",
    "\\text{Sensibilidad} = \\frac{\\text{VP}}{\\text{VP} + \\text{FN}}\n",
    "$\n",
    "\n",
    "**Explicación Conceptual**: \n",
    "\n",
    "- La sensibilidad nos dice cuán bien el modelo es capaz de identificar los casos positivos. \n",
    "- Es crucial en situaciones donde es importante no pasar por alto ningún caso positivo (minimizar falsos negativos). \n",
    "- Un modelo con alta sensibilidad identifica la mayoría de los casos positivos, aunque puede tener un mayor número de falsos positivos.\n",
    "\n",
    "### 4. Especificidad (Specificity) o Tasa de Verdaderos Negativos (TNR)\n",
    "**Definición**: La especificidad mide la proporción de verdaderos negativos entre todos los casos negativos reales.\n",
    "\n",
    "**Fórmula**:\n",
    "\n",
    "$\n",
    "\\text{Especificidad} = \\frac{\\text{VN}}{\\text{VN} + \\text{FP}}\n",
    "$\n",
    "\n",
    "**Explicación Conceptual**: \n",
    "\n",
    "- La especificidad nos dice cuán bien el modelo es capaz de identificar los casos negativos. \n",
    "- Es importante en contextos donde es crucial minimizar los falsos positivos. \n",
    "- Un modelo con alta especificidad predice correctamente la mayoría de los casos negativos, aunque podría fallar en identificar algunos casos positivos.\n",
    "\n",
    "### 5. Puntuación F1 (F1 Score)\n",
    "**Definición**: La puntuación F1 es la media armónica de la precisión positiva y la sensibilidad.\n",
    "\n",
    "**Fórmula**:\n",
    "\n",
    "$\n",
    "\\text{F1 Score} = \\frac{2 \\cdot \\text{Precisión Positiva} \\cdot \\text{Sensibilidad}}{\\text{Precisión Positiva} + \\text{Sensibilidad}}\n",
    "$\n",
    "\n",
    "**Explicación Conceptual**: \n",
    "\n",
    "- La puntuación F1 proporciona un balance entre la precisión positiva y la sensibilidad, y es útil cuando hay un desbalance entre las clases. \n",
    "- La media armónica penaliza los valores extremos más que la media aritmética, asegurando que ambos valores (precisión y sensibilidad) sean razonablemente altos. \n",
    "- Es especialmente útil en situaciones donde tanto los falsos positivos como los falsos negativos tienen un costo significativo y es necesario un balance entre ambos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Interpretación de las resultados del ejemplo\n",
    "\n",
    "\n",
    "- **Precisión (Accuracy)**: 0.85 indica que el 85% de las predicciones del modelo fueron correctas.\n",
    "- **Precisión Positiva (Precision)**: 0.91 indica que cuando el modelo predice que alguien tiene cáncer, es correcto el 91% de las veces.\n",
    "- **Sensibilidad (Recall)**: 0.83 indica que el 83% de las personas que realmente tienen cáncer fueron correctamente identificadas por el modelo.\n",
    "- **Especificidad (Specificity)**: 0.88 indica que el 88% de las personas que no tienen cáncer fueron correctamente identificadas por el modelo.\n",
    "- **Puntuación F1 (F1 Score)**: 0.87 es una medida equilibrada que considera tanto la precisión como la sensibilidad, útil cuando hay un desequilibrio entre clases.\n",
    "\n",
    "Estas métricas ayudan a evaluar el rendimiento del modelo de clasificación desde diferentes perspectivas, proporcionando una comprensión más completa de su efectividad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##¢ Es importante comprender:\n",
    "\n",
    "1. **Pregunta**: ¿Qué métrica sería más importante si queremos minimizar el número de personas con cáncer que no son diagnosticadas correctamente (falsos negativos)?\n",
    "   - **Explicación**: En el contexto del diagnóstico de cáncer, es crítico identificar correctamente a los pacientes con cáncer para que puedan recibir tratamiento. \n",
    "   - La **sensibilidad (recall)** mide la proporción de verdaderos positivos (personas con cáncer correctamente diagnosticadas) entre todos los casos reales de cáncer, por lo que es una métrica clave para este objetivo.\n",
    "\n",
    "2. **Pregunta**: ¿Por qué la precisión positiva (precision) podría ser importante cuando se considera el diagnóstico de cáncer, y cómo se relaciona con los falsos positivos?\n",
    "   - **Explicación**: La **precisión positiva (precision)** mide la proporción de verdaderos positivos entre todas las predicciones positivas. \n",
    "   - Es importante porque un alto número de falsos positivos (personas sin cáncer diagnosticadas incorrectamente como si tuvieran cáncer) podría causar ansiedad innecesaria y llevar a pruebas adicionales costosas e invasivas. \n",
    "   - La precisión positiva nos ayuda a entender cuán confiables son nuestras predicciones positivas.\n",
    "\n",
    "3. **Pregunta**: ¿Qué consecuencias podría tener un modelo con alta especificidad pero baja sensibilidad en el diagnóstico de cáncer?\n",
    "   - **Explicación**: La **especificidad** mide la proporción de verdaderos negativos (personas sin cáncer correctamente diagnosticadas) entre todos los casos reales sin cáncer. \n",
    "   - Un modelo con alta especificidad pero baja sensibilidad identificaría correctamente a la mayoría de las personas sin cáncer, pero fallaría en diagnosticar a muchas personas con cáncer. \n",
    "   - Esto podría resultar en una falta de tratamiento para pacientes con cáncer, lo cual es muy peligroso.\n",
    "\n",
    "4. **Pregunta**: En el contexto del diagnóstico de cáncer, ¿por qué podría ser preferible una puntuación F1 alta en comparación con solo una alta precisión o sensibilidad?\n",
    "   - **Explicación**: La **puntuación F1** es la media armónica de la precisión y la sensibilidad, proporcionando un balance entre ambas. \n",
    "   - En el diagnóstico de cáncer, tanto identificar correctamente a los pacientes con cáncer (alta sensibilidad) como evitar diagnósticos incorrectos de cáncer (alta precisión) son importantes. Una puntuación F1 alta indica un buen balance entre ambas métricas, lo cual es crucial para un diagnóstico confiable y efectivo.\n",
    "\n",
    "5. **Pregunta**: ¿Cómo podría afectar la prevalencia del cáncer en la población (proporción de casos positivos) la elección de la métrica más relevante?\n",
    "   - **Explicación**: Si el cáncer es relativamente raro en la población, \n",
    "   - la **precisión** (accuracy) podría ser alta simplemente porque la mayoría de las personas no tienen cáncer. \n",
    "   - Sin embargo, en tal caso, métricas como la **sensibilidad** y la **precisión positiva** se vuelven más críticas, ya que queremos asegurar que no estamos pasando por alto casos de cáncer ni generando muchos falsos positivos.\n",
    "\n",
    "6. **Pregunta**: Si el objetivo del modelo es garantizar que casi todas las personas con cáncer sean identificadas, incluso si eso significa que algunas personas sin cáncer sean diagnosticadas incorrectamente, ¿qué métrica sería la más relevante?\n",
    "   - **Explicación**: En este caso, la **sensibilidad (recall)** sería la métrica más relevante, ya que mide la capacidad del modelo para identificar correctamente todos los casos de cáncer, minimizando los falsos negativos. \n",
    "   - Este enfoque puede ser crucial en situaciones donde es mejor realizar más pruebas adicionales a costa de tener algunos falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
